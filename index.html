<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to automating quality assurance</title>
    <meta charset="utf-8" />
    <meta name="author" content="Statistics Development Team" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to automating quality assurance
## Official statistics in DfE
### Statistics Development Team
### 2021/06/10

---









# What we'll cover

???

Remember to record it!!

Introduce selves

Follows on from introduction to RAP that we did around 6 months ago, that's available on our guidance website

--

- What do we mean by 'automated QA'

--

- Why we should use automation in our QA

--

- How to get started

--

- Examples of this in action 

--

- What support is available

--

- Time for questions

---
layout: true

# What is automated QA?

---

--

Writing out our quality assurance processes into code so the computer does the legwork

???

We do QA already

This is all about improving what we already do

Using code and the power of computers to improve our current processes

---

Two main types of QA that we have:

&lt;br&gt;

--

Pass/Fail tests

???

I.e. is a specific variable present?

--

- Computers excel at this, and we should fully automate these checks in code

???

Data screener is a great example of this

Incredibly efficient - around 70 checks that are ran in an instant (depending on file size)

Thoroughly documented - everything is stored in code

Reliable - runs the same checks time after time

Accurate - no human error like we had in the early days when Tom or I were manually checking files and missing things

Automation is really powerful here, but this is only half of the story

---

Two main types of QA that we have:

&lt;br&gt;

Sense checking

???

I.e. are the year on year changes in the data feasible, or does this imply a quality issue we need to investigate

--

- Will always require statistician expertise and judgment

--

- We should use code to speed up getting the information we need

--

- We should use code to create visualisations to aid us to do more with this

---

It's a key part of Reproducible Analytical Pipelines (RAP):


&lt;img src = "images/hex-diagram.svg" /&gt;

???

These are the RAP levels we're all working towards

As David laid out in his email to DISD yesterday, we're aiming to meet all of good and great practice before 2022 production cycles

Worth mentioning that this is best prepared before the production cycle starts, so we should be working towards these now

---

It's a key part of Reproducible Analytical Pipelines (RAP):


&lt;img src = "images/hex-diagram-focussed.svg" /&gt;

???

Red - RAP levels that this directly hits

Purple - RAP levels that are indirectly worked towards by doing this

Highlights how critical this is to RAP


---

layout: true

# Why automate our QA

---

--

- Reliability

???

Manual steps in production and quality assurance introduce human error – everybody makes mistakes

--

- Efficiency

???

Computers can do a lot of the leg work much quicker than we can

Allows us to rerun at the last minute

--

- Reproducibility

???

Automating it documents it

--

- Thoroughness

???

By automating our QA, we expand the possible checks that we can do on the data

--

- Consistency

???

Consistent QA across all our publications

Can make use of template code to save duplication of effort

---

Because we're expected to:

--

- Priority in DISD

???

David's email around RAP ambitions - DISD

Next week we're talking with statistics leaders across DfE to support this more widely

--

- Office for Statistics Regulation

???

Have said that they'll be including RAP when they assess the badging of publications

--

- Code of Practice, e.g Q3: Assured quality

&gt; Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.

???

QA is a massive part of the code and how we produce statistics

Leads onto the next point

---

Be the best we can be:

--

- We should be aiming to do the best job of producing statistics that we can

--

- Be the leaders of this across GSS

--

- Help to improve the quality and trustworthiness of government statistics as a whole

???

Sounds like a stretch, but is genuinely possible

If we do this well, we can then share this around the community and bring everyone with us

It's already happening with OGD's copying the self-assessment app and RAP levels approach

Chances for us to share code and examples, making use of Slack etc

Part of a bigger thing and it's genuinely really exciting

Enough of me talking - pass to Sarah

---
layout: true

# Why automate your QA?

---


### Out with the old...


.pull-left[

- How do you answer the question **“am I happy that all my checks pass?”**


- If your QA is **not** automated, this will involve running a check, reviewing the result and amending if needed


]

.pull-right[


![Flowchart of manual QA](images/Old QA SEN2.jpg)


]

???
Example here with SEN2 release - previously had to manually run the same check for age group across all indicators in three different files

---
layout: true

# Why automate your QA?

---

### In with the new!

.center[![Flowchart of automated QA](images/NEW QA SEN2.jpg)]


--

- How do you answer the question **“am I happy that all my checks pass?”** once you've automated QA?

--

- With automated QA, you can write generalisable code and functions, allowing you to check multiple things at once and review the outcomes of checks in one go

???
You can quickly see if any checks fail, make the relevant amendments and run again.


SEN2 example - only need to write the check once, then apply it across everything


putting the resource into doing it right first will pay off in years to come


---
layout: false

# How to get started

[ Thought: In this section we should start by introducing the guidance website, and then running through the checks in that. For each check we'd pose the rhetorical question of how it's done manually, and then also show the template code and how it's possible do those checks in R. ]

--

- Once you have R and Rstudio downloaded, you can begin automating QA.

--

- We have some template code on [GitHub](https://github.com/dfe-analytical-services/automated-data-qa) that can be used to run some [suggested basic checks](https://rsconnect/rsc/stats-production-guidance/rap.html#Basic_automated_QA).


--
- These cover basic checks like minimum/maximum/average values for indicators, counts for suppressed numbers, checks for duplicates

--

- There are also some templates for scatter plots to look at comparing data by time period

--

- You’ll want to include more specific tests to your data too, e.g.

--
+ Do filter subtotals add up to totals?
+ Are percentages and averages calculated correctly?
+ How do indicators change when you apply various filters?

---

# Applying templates


```
check_LA_region_totals &lt;- function(indicator) 

{
function_definition_here %&gt;% filter(indicator)
}

```
--

- Our templates are written as functions that you can apply to your tidy data

--

- The arguments the function takes are defined inside the function brackets **function()**

--

- To run the code, you just need to tell the R what you want variable you want it to look at, e.g.:

```
check_LA_region_totals(number_on_roll)

```

---

# Examples in DISD

---

## SEN 2

- Automating QA as a part of a larger piece of work to automate the end-to-end pipeline for their release

--

- Quality assurance checks at multiple steps of the pipeline: data entry, calculation of indicators, csv outputs

--

- First team to reach “best practice” in RAP so far

--

- Updates to the “variable_change” file have made it easy to transition across for this year

--

###[Demo of SEN2 QA report](examples/QA_2020_output.html)

---

# Data screener app

[ Thought: do we want to do something demo'ing all the cool stuff we've done here, it's an easy way for them to get started on this and also to see the power of it. Though also reaffirm that the end goal is that they use the template code in their processes. Even the screener part of the app will eventually be put into an R package and teams will use that in code scripts in their pipeline. ]

---

---
layout: false

# QA-ing your QA code

???

As code gets more complex, the question of trust in the code comes up

This is more advanced, but hopefully gives a sense of what is possible in the direction we're headed

--

Get your code peer reviewed 

???

Part of RAP levels

--

Version control is key - use Git for tracking all updates and changes to code

???

Part of our best practice for RAP

--

Write 'unit tests' on your QA code using the [testthat](https://testthat.r-lib.org/) package

???

Definitely more advanced, and in terms of RAP, this is the next level above our current best practice

--

- Recent [coffee and coding session](https://web.microsoftstream.com/video/6806b4ba-7b13-4c3c-96f1-9b91c0f1d85b?referrer=https:%2F%2Feducationgovuk.sharepoint.com%2F) on testing code

--

- The tests will use dummy data and check that the code still works as intended

--

- Automatically run these tests after any changes to the code

---

# Support

--

RAP page of the guidance website

- https://rsconnect/rsc/stats-production-guidance/rap.html#Basic_automated_QA

--

Template code repository

- https://github.com/dfe-analytical-services/automated-data-qa/tree/main/R

--

Expansion of data-screener app

- https://rsconnect-pp/rsc/dfe-published-data-qa/

???

This is the pre-production version, where you can test the next features for the next couple of weeks before we fully deploy them

--

Our team 

- [statistics.development@education.gov.uk](mailto:statistics.development@education.gov.uk)

---

# Partnership programme


Statistics Development Team (us) working alongside your team to agree goals and deliver over a set preiod of time. E.g.

--

- Help up-skill your team in using SQL and R

--

- Help to transition certain processes into R

--

- Work your way up the [RAP levels](https://rsconnect/rsc/publication-self-assessment/)

--

- Developing a dashboard to sit alongside EES publications

--

- Getting start with automating your QA

--

Anything related to statistics publications - we're a flexible resource to make use of!


---
layout: true

# Next steps

---

--

Read the guidance and get started

--

Test out the extra features we're adding to the screener

???

There's a separate link that Sarah showed earlier, will be in the guidance for testing this

--

Make use of the template code

???

Make sure you're covering the checks we have code for

Use that as a base to expand from, develop your own checks above the basic ones to meet the great practice level

---

Continue work towards goal of good and great practice RAP

???

Those in DISD will have seen David's email yesterday, this is a key aim for our division over the next few months

--

- Good practice = doing the basic checks we outline in the guidance

--

- Great practice = building on these to automate publication specific checks

---

For us as a support team:

--

- Helping with lots of questions and partnership programmes

--

- Developing the guidance further

???

Please share any examples you have of what you're doing and we can add that in for other teams to learn from

--

- Expanding and improving template code

--

- Expanding and improving the extra features in the screener

--

- Packaging up the screener app and template code

???

Long term goal that everything will be functions that you can all use in your R pipelines

--

- Running further sessions

???

Let us know which parts of RAP you want to know more about

--

- Tracking RAP progress in self-assessment app

???

Targeting meeting all of good and great practice BEFORE 2022 publication cycles

Make use of the app to communicate L+D needs, or areas of RAP that need more support


---

Longer term:

- You can look at using peer review, version control and unit tests to ensure your QA is robust

--

- Template code, the screening checks, all will be packaged up to use in pipelines

???

Moving to a fully automated model

End to end R pipelines

The pipeline will write the files direct to the EES admin databases

This is the direction we're aiming for, though don't worry if it sounds scary 

It's still a few years away and we're taking it step by step

---

layout: false

# Final links

If you have any questions or are interested in our partnership programme, please get in touch:

[statistics.development@education.gov.uk](mailto:statistics.development@education.gov.uk)

--

Today's slides

https://sarahmwong.github.io/intro-to-automating-QA/

???

You'll be able to find these, and the recording of the session on the guidance website by the end of today

--

Made in R using the [xaringan](https://bookdown.org/yihui/rmarkdown/xaringan.html) package

--

Guidance website:

https://rsconnect/rsc/stats-production-guidance/rap.html#Basic_automated_QA

???

Includes slides and recording of the introduction to RAP session

Also links to everything that we've mentioned and shown today

--

Any questions?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
